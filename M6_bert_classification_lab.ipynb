{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65571921",
   "metadata": {
    "id": "65571921"
   },
   "source": [
    "# BBC News Classification with BERT: Lab\n",
    "# Solution\n",
    "\n",
    "## Overview\n",
    "As a junior data scientist at NewsInsight, a media analytics company, you've been tasked with building an automated news categorization system. Your team needs to classify incoming news articles into appropriate categories to help journalists, researchers, and business analysts quickly find relevant information.\n",
    "\n",
    "The company receives thousands of articles daily from various sources. Currently, human editors spend significant time manually categorizing these articles, which is time-consuming and inconsistent. Your manager has asked you to develop a machine learning solution that can automatically categorize news articles into predefined categories (business, entertainment, politics, sport, tech).\n",
    "\n",
    "This project will follow the BERT fine-tuning process you've learned:\n",
    "1. Understanding data and defining requirements\n",
    "2. Selecting and preparing the BERT model\n",
    "3. Data preparation and tokenization\n",
    "4. Model architecture design\n",
    "5. Fine-tuning the model\n",
    "6. Evaluation and refinement\n",
    "\n",
    "Successfully implementing this system will significantly improve workflow efficiency, allowing editors to focus on content quality rather than manual categorization.\n",
    "\n",
    "## Part 1: Environment Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7a44d",
   "metadata": {
    "id": "d5d7a44d"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c989fc2",
   "metadata": {
    "id": "2c989fc2"
   },
   "outputs": [],
   "source": [
    "# Load the data from csv file\n",
    "df = None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a4901",
   "metadata": {
    "id": "8b9a4901"
   },
   "source": [
    "## Part 2: Data Exploration and Preprocessing\n",
    "\n",
    "Explore the dataset, displaying basic information and:\n",
    "- Analyze category distribution\n",
    "- Check text length distribution\n",
    "- Train test split data, use 75-25 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c59aac",
   "metadata": {
    "id": "83c59aac"
   },
   "outputs": [],
   "source": [
    "# Explore the first 10 rows\n",
    "None\n",
    "\n",
    "# Basic info and describe\n",
    "None\n",
    "\n",
    "# Category distribution\n",
    "category_counts = None\n",
    "category_counts\n",
    "\n",
    "# Visualize category distribution\n",
    "None\n",
    "\n",
    "# Text length analysis\n",
    "df['text_length'] = None\n",
    "print(f\"Average text length: {df['text_length'].mean()}\")\n",
    "print(f\"Min text length: {df['text_length'].min()}\")\n",
    "print(f\"Max text length: {df['text_length'].max()}\")\n",
    "\n",
    "# Visualize text length by category\n",
    "None\n",
    "\n",
    "# Check for missing values\n",
    "None\n",
    "\n",
    "# Map category labels to integers for classification\n",
    "categories = list(df['category'].unique())\n",
    "category_mapping = {category: i for i, category in enumerate(categories)}\n",
    "df['label'] = df['category'].map(category_mapping)\n",
    "\n",
    "# Rename content column into text for hugging face\n",
    "df.rename(columns={'content': 'text'}, inplace=True)\n",
    "\n",
    "# Split into train, validation, and test sets, make sure to stratify based on category, keep features and target together for now\n",
    "# First, create train+validation and test sets\n",
    "train_val_df, test_df = None\n",
    "# Then split train+validation into separate train and validation sets\n",
    "train_df, val_df = None\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Ensure categories are distributed properly across splits\n",
    "print(\"\\nCategory distribution in training set:\")\n",
    "print(train_df['category'].value_counts())\n",
    "print(\"\\nCategory distribution in validation set:\")\n",
    "print(val_df['category'].value_counts())\n",
    "print(\"\\nCategory distribution in test set:\")\n",
    "print(test_df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db3cf5",
   "metadata": {
    "id": "65db3cf5"
   },
   "source": [
    "## Part 3: Choose Your Model Approach\n",
    "You can implement either the TensorFlow approach OR the Hugging Face approach. Delete the one you do not use.\n",
    "\n",
    "### ------ TensorFlow Approach --------\n",
    "Implement BERT with TensorFlow and TensorFlow Hub\n",
    "- Import required libraries\n",
    "- Select and load a BERT model\n",
    "- Create datasets\n",
    " - Build model architecture\n",
    " - Fine-tune the model\n",
    " - Evaluate performance\n",
    " - Create visuals for train and validation data metrics across epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa516c",
   "metadata": {
    "id": "75fa516c"
   },
   "outputs": [],
   "source": [
    "# Make sure to set legacy Keras to work with TF Hub BERT before you import\n",
    "os.environ['TF_USE_LEGACY_KERAS']= '1'\n",
    "\n",
    "# Import TensorFlow-specific libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "# Select and load BERT model\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "def create_tf_dataset(texts, labels, batch_size=32, shuffle=True):\n",
    "    None\n",
    "\n",
    "# Convert pandas DataFrames to TensorFlow datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "# Make sure to not shuffle for test data\n",
    "test_dataset = None\n",
    "\n",
    "# Build the BERT model\n",
    "def build_tf_classifier_model():\n",
    "    # Text input\n",
    "    None\n",
    "\n",
    "    # Preprocessing layer\n",
    "    None\n",
    "\n",
    "    # BERT encoder - set trainable=True for fine-tuning\n",
    "    None\n",
    "\n",
    "    # Use the pooled output for classification\n",
    "    None\n",
    "\n",
    "    # Add dropout for regularization\n",
    "    None\n",
    "\n",
    "    # Add classification layer (for 5 categories)\n",
    "    None\n",
    "\n",
    "    # Create model\n",
    "    model = None\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "tf_classifier_model = build_tf_classifier_model()\n",
    "tf_classifier_model.summary()\n",
    "\n",
    "# Compile the model\n",
    "# Using sparse categorical crossentropy since our labels are integers\n",
    "loss = None\n",
    "# Select accuracy as metric\n",
    "metrics = None\n",
    "\n",
    "# Set up learning rate and optimizer\n",
    "init_lr = .0005\n",
    "optimizer = None\n",
    "\n",
    "# Compile the model\n",
    "None\n",
    "\n",
    "# Set up early stopping callback based on validation accuracy\n",
    "early_stopping = None\n",
    "\n",
    "\n",
    "# Train the model for 5 epochs (not enough epochs most likely but to save on time)\n",
    "print('Fine-tuning BERT model...')\n",
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebc4e2",
   "metadata": {
    "id": "d2ebc4e2"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on testing data\n",
    "test_loss, test_accuracy = None\n",
    "print(f'Test accuracy (TensorFlow): {test_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2f5bf",
   "metadata": {
    "id": "87c2f5bf"
   },
   "source": [
    "### ------- Hugging Face Approach -------\n",
    "Implement BERT with Hugging Face Transformers\n",
    "- Import required libraries\n",
    "- Select and load a BERT model\n",
    "- Tokenize data\n",
    "- Create datasets\n",
    "- Fine-tune the model\n",
    "- Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52f9a2",
   "metadata": {
    "id": "ec52f9a2"
   },
   "outputs": [],
   "source": [
    "# Import Hugging Face libraries\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Select and load tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = None\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face datasets\n",
    "train_dataset_hf = Dataset.from_pandas(train_df)\n",
    "val_dataset_hf = Dataset.from_pandas(val_df)\n",
    "test_dataset_hf = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenize function (use 128 for max length)\n",
    "None\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = None\n",
    "tokenized_val = None\n",
    "tokenized_test = None\n",
    "\n",
    "# Load pre-trained model with classification head\n",
    "model_hf = None\n",
    "# Set BERT encoder layers to not train\n",
    "None\n",
    "\n",
    "# Define metrics computation function the returns a dictionary of scores - include at least accuracy\n",
    "def compute_metrics(pred):\n",
    "    None\n",
    "\n",
    "# Set up training arguments (train for 5 epochs - not enough to fully train but for sake of time. Set learning rate of 0.0005)\n",
    "training_args = None\n",
    "\n",
    "# Initialize Trainer - include an early stopping callback with patience of 3\n",
    "trainer = None\n",
    "\n",
    "# Train the model\n",
    "print('Fine-tuning BERT model with Hugging Face...')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed0719",
   "metadata": {
    "id": "16ed0719"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = None\n",
    "print(f\"Hugging Face Model Results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b84ef",
   "metadata": {
    "id": "5f7b84ef"
   },
   "source": [
    "## Part 4: Model Analysis and Inference\n",
    "Analyze model performance on testing data\n",
    "- Create confusion matrix visualization\n",
    "- Analyze misclassifications\n",
    "- Identify strengths and weaknesses\n",
    "\n",
    "### Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5691ba",
   "metadata": {
    "id": "8d5691ba"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for the test set\n",
    "test_predictions = None\n",
    "y_pred = None\n",
    "\n",
    "# Get true labels from testing data\n",
    "y_true = [labels.numpy() for _, labels in test_dataset.unbatch()]\n",
    "\n",
    "# Create and visualize the confusion matrix\n",
    "cm = None\n",
    "plt.figure(figsize=(10, 8))\n",
    "disp = None\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix for BBC News Classification (TensorFlow)')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Create classification report\n",
    "report = None\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=categories))\n",
    "\n",
    "# Identify strengths and weaknesses\n",
    "print(\"\\nModel Strengths and Weaknesses:\")\n",
    "\n",
    "# Calculate per-class metrics (use classification report)\n",
    "per_class_metrics = {}\n",
    "None\n",
    "\n",
    "# Find best and worst performing categories\n",
    "best_category = None\n",
    "worst_category = None\n",
    "\n",
    "print(f\"\\nStrengths:\")\n",
    "print(f\"- Overall accuracy: {report['accuracy']:.4f}\")\n",
    "print(f\"- Best performing category: {best_category[0]}\")\n",
    "print(f\"\\nWeaknesses:\")\n",
    "print(f\"- Worst performing category: {worst_category[0]}\")\n",
    "\n",
    "# Visualize per-class performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "categories_indices = range(len(categories))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar([i - width for i in categories_indices],\n",
    "        [per_class_metrics[cat]['precision'] for cat in categories],\n",
    "        width=width, label='Precision')\n",
    "plt.bar(categories_indices,\n",
    "        [per_class_metrics[cat]['recall'] for cat in categories],\n",
    "        width=width, label='Recall')\n",
    "plt.bar([i + width for i in categories_indices],\n",
    "        [per_class_metrics[cat]['f1-score'] for cat in categories],\n",
    "        width=width, label='F1-Score')\n",
    "\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics by Category')\n",
    "plt.xticks(categories_indices, categories, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115cc66",
   "metadata": {
    "id": "9115cc66"
   },
   "outputs": [],
   "source": [
    "# Create a function for model inference on new articles\n",
    "def predict_article_category(text, model=tf_classifier_model):\n",
    "    \"\"\"\n",
    "    Predict the category of a news article using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text of the news article\n",
    "        model: The fine-tuned TensorFlow model\n",
    "\n",
    "    Returns:\n",
    "        dict: Prediction results including category and confidence scores\n",
    "    \"\"\"\n",
    "    # Make prediction\n",
    "    prediction = None\n",
    "\n",
    "    # Get the predicted category and confidence\n",
    "    predicted_class_id = None\n",
    "    predicted_category = categories[predicted_class_id]\n",
    "    confidence = float(prediction[predicted_class_id])\n",
    "\n",
    "    # Get confidence for all categories\n",
    "    category_confidences = {categories[i]: float(prediction[i]) for i in range(len(categories))}\n",
    "\n",
    "    # Sort categories by confidence (descending)\n",
    "    sorted_categories = sorted(category_confidences.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return {\n",
    "        'text': text[:100] + '...' if len(text) > 100 else text,\n",
    "        'predicted_category': predicted_category,\n",
    "        'confidence': confidence,\n",
    "        'all_confidences': sorted_categories\n",
    "    }\n",
    "\n",
    "# Test the inference function with example articles\n",
    "sample_articles = [\n",
    "    \"The tech giant announced the release of their new smartphone that features advanced AI capabilities and improved battery life. The product will be available in stores next month.\",\n",
    "    \"The football team secured their victory in the final minutes with a spectacular goal. The win puts them at the top of the league table.\",\n",
    "    \"Stock markets plummeted following the central bank's announcement of interest rate increases. Investors are concerned about the impact on economic growth.\",\n",
    "    \"The new film starring the award-winning actress has received critical acclaim at the international film festival. Critics praised the innovative cinematography.\",\n",
    "    \"The government announced new policies regarding digital privacy and data protection. Opposition parties have criticized the measures as inadequate.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting inference on sample articles:\")\n",
    "for i, article in enumerate(sample_articles):\n",
    "    result = predict_article_category(article)\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Predicted category: {result['predicted_category']} (confidence: {result['confidence']:.4f})\")\n",
    "    print(\"All category confidences:\")\n",
    "    for category, conf in result['all_confidences']:\n",
    "        print(f\"  - {category}: {conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d541f",
   "metadata": {
    "id": "fc5d541f"
   },
   "source": [
    "### Hugging Face Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff0465b",
   "metadata": {
    "id": "9ff0465b"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for the test set\n",
    "def get_predictions(trainer, dataset):\n",
    "    # Run predictions with Hugging Face Trainer\n",
    "    raw_predictions = None\n",
    "\n",
    "    # Extract predictions and labels\n",
    "    predictions = None\n",
    "    labels = None\n",
    "\n",
    "    return predictions, labels\n",
    "\n",
    "y_pred, y_true = get_predictions(trainer, tokenized_test)\n",
    "\n",
    "# Create and visualize the confusion matrix\n",
    "cm = None\n",
    "plt.figure(figsize=(10, 8))\n",
    "disp = None\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix for BBC News Classification (Hugging Face)')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Create classification report\n",
    "report = None\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=categories))\n",
    "\n",
    "\n",
    "# Identify strengths and weaknesses\n",
    "print(\"\\nModel Strengths and Weaknesses:\")\n",
    "\n",
    "# Calculate per-class metrics (use classification report)\n",
    "per_class_metrics = {}\n",
    "None\n",
    "\n",
    "# Find best and worst performing categories\n",
    "best_category = None\n",
    "worst_category = None\n",
    "\n",
    "print(f\"\\nStrengths:\")\n",
    "print(f\"- Overall accuracy: {report['accuracy']:.4f}\")\n",
    "print(f\"- Best performing category: {best_category[0]}\")\n",
    "print(f\"\\nWeaknesses:\")\n",
    "print(f\"- Worst performing category: {worst_category[0]}\")\n",
    "\n",
    "# Visualize per-class performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "categories_indices = range(len(categories))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar([i - width for i in categories_indices],\n",
    "        [per_class_metrics[cat]['precision'] for cat in categories],\n",
    "        width=width, label='Precision')\n",
    "plt.bar(categories_indices,\n",
    "        [per_class_metrics[cat]['recall'] for cat in categories],\n",
    "        width=width, label='Recall')\n",
    "plt.bar([i + width for i in categories_indices],\n",
    "        [per_class_metrics[cat]['f1-score'] for cat in categories],\n",
    "        width=width, label='F1-Score')\n",
    "\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics by Category')\n",
    "plt.xticks(categories_indices, categories, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f2b7a",
   "metadata": {
    "id": "759f2b7a"
   },
   "outputs": [],
   "source": [
    "# Create a function for model inference on new articles\n",
    "def predict_article_category(text, model=model_hf, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Predict the category of a news article using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text of the news article\n",
    "        model: The fine-tuned Hugging Face model\n",
    "        tokenizer: The tokenizer for the model\n",
    "\n",
    "    Returns:\n",
    "        dict: Prediction results including category and confidence scores\n",
    "    \"\"\"\n",
    "    # Tokenize inputs\n",
    "    inputs = None\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        None\n",
    "\n",
    "    # Convert to numpy for easier handling\n",
    "    probs = None\n",
    "\n",
    "    # Get the predicted category and confidence\n",
    "    predicted_class_id = None\n",
    "    predicted_category = categories[predicted_class_id]\n",
    "    confidence = float(probs[predicted_class_id])\n",
    "\n",
    "    # Get confidence for all categories\n",
    "    category_confidences = {categories[i]: float(probs[i]) for i in range(len(categories))}\n",
    "\n",
    "    # Sort categories by confidence (descending)\n",
    "    sorted_categories = sorted(category_confidences.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return {\n",
    "        'text': text[:100] + '...' if len(text) > 100 else text,\n",
    "        'predicted_category': predicted_category,\n",
    "        'confidence': confidence,\n",
    "        'all_confidences': sorted_categories\n",
    "    }\n",
    "\n",
    "# Test the inference function with example articles\n",
    "sample_articles = [\n",
    "    \"The tech giant announced the release of their new smartphone that features advanced AI capabilities and improved battery life. The product will be available in stores next month.\",\n",
    "    \"The football team secured their victory in the final minutes with a spectacular goal. The win puts them at the top of the league table.\",\n",
    "    \"Stock markets plummeted following the central bank's announcement of interest rate increases. Investors are concerned about the impact on economic growth.\",\n",
    "    \"The new film starring the award-winning actress has received critical acclaim at the international film festival. Critics praised the innovative cinematography.\",\n",
    "    \"The government announced new policies regarding digital privacy and data protection. Opposition parties have criticized the measures as inadequate.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting inference on sample articles:\")\n",
    "for i, article in enumerate(sample_articles):\n",
    "    result = predict_article_category(article)\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Predicted category: {result['predicted_category']} (confidence: {result['confidence']:.4f})\")\n",
    "    print(\"All category confidences:\")\n",
    "    for category, conf in result['all_confidences']:\n",
    "        print(f\"  - {category}: {conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab4efb",
   "metadata": {
    "id": "18ab4efb"
   },
   "source": [
    "# Part 5: Conclusion and Discussion\n",
    "Summarize your findings\n",
    "- What was the final accuracy and other metrics?\n",
    "- What categories were easiest/hardest to classify?\n",
    "- What challenges did you encounter?\n",
    "- How might you improve the model further?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
